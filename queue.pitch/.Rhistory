m <- xgetinverse()
m <- getinverse()
m <- NULL
m <- getinverse()
if(!is.null(m)) {
message("getting cached data")
return(m)
}
data <- x$get()
x <- test
makeCacheMatrix <- function(x = matrix()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setinverse <- function(solve) m <<- solve
getinverse <- function() m
list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
}
makeCacheMatrix(test)
cacheSolve(test)
x <- makeCacheMatrix()
cacheSolve(test)
x$set(matrix(1:4,2,2))
cacheSolve(test)
cacheSolve(x)
x
x$set
m <- x$getinverse()
data <- x$get()
m <- solve(data, ...)
View(m)
x$setinverse(m)
m
a$set(test)
x$set(test)
cacheSolve <- function(x, ...) {
m <- x$getinverse()
if(!is.null(m)) {
message("getting cached data")
return(m)
}
data <- x$get()
m <- solve(data, ...)
x$setinverse(m)
m
}
cacheSolve(test)
cacheSolve(x)
library(wes.anderson)
library(wesanderson)
install.packages("wesanderson")
library(wesanderson)
?wes.pallete
?wes.palette
colors <- wes.palette('Zissou')
colors <- wes.palette(name = 'Zissou')
colors <- wes.palette(5,name = 'Zissou')
colors <- wes.palette(6,name = 'Zissou')
colors <- wes.palette(5,name = 'Zissou')
colors
wes.palette(5, "Moonrise3")
display.wes.palette(5, "Moonrise3")
display.wes.palette(4, "Chevalier")
wes.palette(4, "Chevalier")
colors
display.wes.palette(5, "Zissou")
display.wes.palette(5, "Moonrise3")
display.wes.palette(5, "Moonrise2")
display.wes.palette(4, "Moonrise2")
wes.palette(5, "Zissou")
wes.palette(4, "Chevalier")
display.wes.palette(4, "Chevalier")
display.wes.palette(5, "Zissou")
colors
display.wes.palette(4, "Chevalier")
wes.palette(4, "Chevalier")
display.wes.palette(4, "Moonrise2")
display.wes.palette(5, "Zissou")
wes.palette(5, "Zissou")
wes.palette
display.wes.palette
set.seed(82)
avatar <- runif(0,82,82)
plot(avatar)
plot(avatar,avatar)
?plot
avatar
avatar <- runif(0,82,82)
?runif
avatar <- runif(82,0,82)
plot(avatar,avatar)
plot(avatar)
plot(avatar,pch = 10,axes = F)
plot(avatar,pch = 10,cex = 10,axes = F)
avatar <- runif(100000,0,82)
plot(avatar,pch = 10,cex = 10,axes = F)
avatar <- runif(10000,0,82)
plot(avatar,pch = 10,cex = 10,axes = F)
avatar <- runif(1000,0,82)
plot(avatar,pch = 10,cex = 10,axes = F)
avatar <- data.frame(num = runif(1000,0,82),color = runif(1000,0,5))
library(ggplot)
library(ggplot2)
?scale_shape
ggplot(avatar,aes(x = num,fill = color,shape = cut)) + geom_point
ggplot(avatar,aes(x = num,fill = color,shape = cut)) + geom_point()
?'::'
?set.seed
set.seed
runif
C_runif
.External
?.External
?data
load(VADeaths)
data(VADeaths)
summary(VADeaths)
str(VADeaths)
?datsets
?datasets
library(help = "datasets")
View(VADeaths)
?install.packages
?rlnorm
?optim
install.packages('KernSmooth')
library(KernSmooth)
rm(list = ls())
rm(list = ls())
setwd('C:/Users/m097845/Google Drive/Scripts/Machine Learning')
library(ggplot2)
library(reshape2)
library(knitr)
?shuttle
rm(list = ls())
data(shuttle)
install.packages('MASS')
library(MASS)
?shuttle
data(shuttle)
load(shuttle)
str(shuttle)
glm(use ~ wind,data = shuttle)
?glm
glm(use ~ wind,data = shuttle,family = 'binomial')
plot(glm(use ~ wind,data = shuttle,family = 'binomial'))
fit1 <- glm(use ~ wind,data = shuttle,family = 'binomial')
summary(fit1)
fit2 <- glm(use ~ magn,data = shuttle,family = 'binomial')
fit2
coef(fit)
coef(fit1)
exp(coef(fit1))
fit2 <- glm(use ~ wind + magn,data = shuttle,family = 'binomial')
summary(fit2)
exp(coef(fit2))
fit1 <- glm(1 - use ~ wind,data = shuttle,family = 'binomial')
summary(fit1)
fit1 <- glm(1 - use ~ wind,data = shuttle,family = 'binomial')
data(InsectSprays)
load(InsectSprays)
summary(InsectSprays)
fit3 <- glm(count ~ spray,data = InsectSprays,family = 'poisson')
summary(fit3)
fit3$coef
exp(fit3$coef)
14.5/1.06
summary(fit3)$coef
fit3 <- glm(count ~ spray - 1,data = InsectSprays,family = 'poisson')
exp(fit3$coef)
fit1 <- glm(use ~ wind - 1,data = shuttle,family = 'binomial')
exp(coef(fit1))
fit1 <- glm(use ~ wind,data = shuttle,family = 'binomial')
exp(coef(fit1))
rm(list = ls())
install.packages('ElemStatLearn')
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- factor(vowel.test$y)
vowel.train$y <- factor(vowel.train$y)
set.seed(33833)
fit.rf <- train(y ~ .,method = 'rf')
fit.gbm <- train(y ~ .,method = 'gbm')
fit.rf <- train(y ~ .,data = vowel.train,method = 'rf')
fit.gbm <- train(y ~ .,data = vowel.train,method = 'gbm')
predict(fit.rf,newdata = vowel.test)
predict(fit.gbm,newdata = vowel.test)
?predict
predict(fit.rf,newdata = vowel.test) == vowel.test
pred = predict(fit.rf,newdata = vowel.test)
pred = vowel.test
pred = predict(fit.rf,newdata = vowel.test)
pred == vowel.test
pred = predict(fit.rf,newdata = vowel.test) == vowel.test$y
sum(pred)/length(pred)
pred2 = predict(fit.gbm,newdata = vowel.test) == vowel.test$y
sum(pred2)/length(pred2)
agree <- predict(fit.rf,newdata = vowel.test) == predict(fit.gbm,newdata = vowel.test)
sum(agree)/length(agree)
agree <- predict(fit.rf,newdata = vowel.test) == predict(fit.gbm,newdata = vowel.test) &
predict(fit.rf,newdata = vowel.test) == vowel.test$y
sum(agree)/length(agree)
agree <- predict(fit.rf,newdata = vowel.test) == predict(fit.gbm,newdata = vowel.test) &
predict(fit.gbm,newdata = vowel.test) == vowel.test$y
sum(agree)/length(agree)
pred2 = predict(fit.gbm,newdata = vowel.test)
pred = predict(fit.rf,newdata = vowel.test
)
both <- data.frame(y = vowel.test$y,pred,pred2)
agree <- predict(fit.gbm,newdata = vowel.test) == vowel.test$y |
predict(fit.rf,newdata = vowel.test) == vowel.test$y
sum(agree)/length(agree)
predict(fit.gbm,newdata = vowel.test)
agree <- predict(fit.rf,newdata = vowel.test) == predict(fit.gbm,newdata = vowel.test)
sum(agree)/length(agree)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
fit.rf <- train(diagosis ~.,data = training,method = 'rf')
fit.rf <- train(diagnosis ~.,data = training,method = 'rf')
set.seed(62433)
fit.rf <- train(diagnosis ~.,data = training,method = 'rf')
fit.gbm <- train(diagnosis ~.,data = training,method = 'gbm')
fit.gbm <- train(diagnosis ~.,data = training,method = 'lda')
stack <- data.frame(predict(fit.rf,newdata = testing),predict(fit.rf,newdata = testing),
predict(fit.rf,newdata = testing),diagnosis = testing$diagnosis)
View(stack)
fit.stack <- train(diagnosis ~.,data = stack,method = 'rf')
fit.stack
predict(fit.stack,newdata = stack)
test <- predict(fit.stack,newdata = stack) == stack$diagnosis
sum(test)/length(test)
set.seed(62433)
fit.rf <- train(diagnosis ~.,data = training,method = 'rf')
fit.gbm <- train(diagnosis ~.,data = training,method = 'gbm')
fit.lda <- train(diagnosis ~.,data = training,method = 'lda')
pred.rf <- predict(fit.rf,newdata = testing)
pred.gbm <- predict(fit.gbm,newdata = testing)
pred.lda <- predict(fit.lda,newdata = testing)
stack <- data.frame(pred.rf,pred.gbm,pred.lda,diagnosis = testing$diagnosis)
fit.stack <- train(diagnosis ~.,data = stack,method = 'rf')
test <- predict(fit.stack,newdata = stack) == stack$diagnosis
sum(test)/length(test)
sum(pred.rf == testing$diagnosis)/nrow(testing)
sum(pred.gbm == testing$diagnosis)/nrow(testing)
sum(pred.lda == testing$diagnosis)/nrow(testing)
View(adData)
adData = data.frame(diagnosis,predictors)
set.seed(3433)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fit.rf <- train(diagnosis ~.,data = training,method = 'rf')
fit.gbm <- train(diagnosis ~.,data = training,method = 'gbm')
fit.lda <- train(diagnosis ~.,data = training,method = 'lda')
pred.rf <- predict(fit.rf,newdata = testing)
pred.gbm <- predict(fit.gbm,newdata = testing)
pred.lda <- predict(fit.lda,newdata = testing)
stack <- data.frame(pred.rf,pred.gbm,pred.lda,diagnosis = testing$diagnosis)
fit.stack <- train(diagnosis ~.,data = stack,method = 'rf')
test <- predict(fit.stack,newdata = stack) == stack$diagnosis
sum(test)/length(test)
sum(pred.rf == testing$diagnosis)/nrow(testing)
sum(pred.gbm == testing$diagnosis)/nrow(testing)
sum(pred.lda == testing$diagnosis)/nrow(testing)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
fit.lasso <- train(CompressiveStrength ~.,data = concrete,method = 'lasso')
plot(fit.lasso)
?plot.enet
plot.enet(fit.lasso)
summary(fit.lasso)
?enet
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit.lasso <- train(CompressiveStrength ~.,data = training,method = 'lasso')
summary(fit.lasso)
plot.enet(fit.lasso)
plot.enet(fit.lasso,xvar = 'step')
install.packages('lubridate')
library(lubridate)
x <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv')
View(x)
dat <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv')
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages('forecast')
library(forecast)
?
?bats
fit <- bats(training$visitsTumblr)
fcast <- forecast(fit)
plot(fcast)
?forecast
fit <- bats(tstrain)
fcast <- forecast(fit)
plot(fcast)
fit <- bats(tstrain)
fcast <- forecast(fit)
plot(fcast)
lines(testing$visitsTumblr,col= 'red')
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
fit <- train(CompressiveStrength ~.,data = training,method = 'svm')
library(e1071)
accurary(fcast,testing)
accuracy(fcast,testing)
accuracy(fcast,testing$visitsTumblr)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages('forecast')
library(forecast)
fit <- bats(tstrain)
fcast <- forecast(fit)
plot(fcast)
lines(testing$visitsTumblr,col= 'red')
accuracy(fcast,testing$visitsTumblr)
install.packages("forecast")
summary(fcast)
fcast <- forecast(fit,nrow(testing))
summary(fcast)
fcast
fcast[,3]
fcast[3]
fcast[95]
$upper
fcast$upper
fcast$upper[,2]
x <- data.frame(fcast$lower[,2],fcast$upper[,2],testing$visitsTumblr)
View(x)
x[,1] < x[,3] & x[,2] > x[,3]
sum(x[,1] < x[,3] & x[,2] > x[,3])
sum(x[,1] < x[,3] & x[,2] > x[,3])/length(x[,3])
library(e1071)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
fit <- train(CompressiveStrength ~.,data = training,method = 'svm')
fit <- svm(CompressiveStrength ~.,data = training)
set.seed(325)
fit <- svm(CompressiveStrength ~.,data = training)
predict(fit,newdat = testing)
pred <- predict(fit,newdat = testing)
rmse(pred,testing$CompressiveStrength)
install.packages('hydroGOF')
library(hydroGOF)
rmse(pred,testing$CompressiveStrength)
data(diabetes)
View(diabetes)
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
View(training)
fit.lasso <- enet(training[,1:8],training[,9],lambda = 0)
View(predictors)
View(training)
fit.lasso <- enet(as.matrix(training[,1:8]),as.matrix(training[,9]),lambda = 0)
plot.enet(fit.lasso,xvar = 'step')
plot.enet(fit.lasso,xvar = 'penalty')
library(foreign)
?foreign
data <- read.dta('D:/Work/AnalyticFileForNilay_090814.dta')
View(data)
summary(data)
getwd()
write.csv(data,'D:/Work/mdi.csv')
setwd('C:/Users/troh/Google Drive/Scripts/Machine Learning')
install.packages('slidify')
install.packages('devtools')
library(devtools)
install_github('slidify,'ramnathv')
install_github('slidify','ramnathv')
?install_github
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
library(slidify)
setwd('C:/Users/troh/Google Drive/Scripts/Machine Learning/Developing Data Products')
author('queue.pitch')
slidify('index.Rmd')
library(knitR)
browseURL('index.html')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
library(queueing)
install.packages('queuing')
install.packages('queueing')
library(queueing)
NewInput.MMC(15,5,4)
model <- NewInput.MMC(15,5,4)
inputs <- NewInput.MMC(15,5,4)
model <- QueueingModel(inputs)
paste('The Average Number of Customers in the System',L(model))
paste('The Average Server Utilization',RO(model))
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
publish(title = 'MMC Queueing', 'index.html', host = 'rpubs')
publish(title = 'MMC Queueing', 'queue.pitch/index.html', host = 'rpubs')
setwd('C:/Users/troh/Google Drive/Scripts/Machine Learning/Developing Data Products/queue.pitch')
publish(title = 'MMC Queueing', 'index.html', host = 'rpubs')
?publish
publish(title = 'MMC.Queueing', 'index.html', host = 'rpubs')
publish_rpubs(title = 'MMC.Queueing', 'index.html')
publish_rpubs(title = 'MMC.Queueing', html_file = 'index.html')
servers <- 4:8
inputs.MMC <- lapply(servers,function(x) NewInput.MMC(15,5,x))
models <- lapply(inputs.MMC,QueueingModel)
results <- data.frame(Servers = as.integer(servers),L = sapply(models,L))
View(results)
ggplot(results,aes(x = Servers,y = L)) +
geom_smooth(method = loess,stat="identity",size = 2,color = '#D8B70A') +
labs(x = 'Number of Servers',y = 'Average # of Customers in System')
library(ggplot2)
servers <- 4:8
inputs.MMC <- lapply(servers,function(x) NewInput.MMC(15,5,x))
models <- lapply(inputs.MMC,QueueingModel)
results <- data.frame(Servers = as.integer(servers),L = sapply(models,L))
ggplot(results,aes(x = Servers,y = L)) +
geom_smooth(method = loess,stat="identity",size = 2,color = '#D8B70A') +
labs(x = 'Number of Servers',y = 'Average # of Customers in System')
slidify('index.Rmd')
browseURL('index.html')
slidify('index.Rmd')
browseURL('index.html')
publish_rpubs(title = 'MMC.Queueing', html_file = 'index.html')
slidify('index.Rmd')
browseURL('index.html')
publish_rpubs(title = 'MMC.Queueing', html_file = 'index.html')
publish_rpubs(title = 'MMC.Queueing', html_file = 'index.html')
tiff('Lplot.tiff')
ggplot(results,aes(x = Servers,y = L)) +
geom_smooth(method = loess,stat="identity",size = 2,color = '#D8B70A') +
labs(x = 'Number of Servers',y = 'Average # of Customers in System')
dev.off()
library(Rmarkdown)
library("rmarkdown", lib.loc="~/R/win-library/3.1")
publish_rpubs(title = 'MMC.Queueing', html_file = 'index.html')
publish_github(repo = 'MMC.App', username = 'datareaver')
publish_github(repo = 'MMC.Pitch', username = 'datareaver')
